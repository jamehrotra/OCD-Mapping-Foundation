{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f603f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading splits from P5_D4_Tsymptom_provocation_SUPENNS001R02.part2.pkl...\n",
      "Saved new splits for P5_D4_Tsymptom_provocation_SUPENNS001R02.part2: 16 train, 1 val, 1 test\n",
      "Loading splits from P5_D4_Tsymptom_provocation_SUPENNS001R02.part3.pkl...\n",
      "Saved new splits for P5_D4_Tsymptom_provocation_SUPENNS001R02.part3: 8 train, 0 val, 0 test\n",
      "Loading splits from P5_D4_Tsymptom_provocation_SUPENNS001R02.pkl...\n",
      "Saved new splits for P5_D4_Tsymptom_provocation_SUPENNS001R02: 17 train, 2 val, 2 test\n",
      "Loading splits from P5_D5_Tsymptom_provocation_SUPENNS001R01.part2.pkl...\n",
      "Saved new splits for P5_D5_Tsymptom_provocation_SUPENNS001R01.part2: 16 train, 2 val, 2 test\n",
      "Loading splits from P5_D5_Tsymptom_provocation_SUPENNS001R01.pkl...\n",
      "Saved new splits for P5_D5_Tsymptom_provocation_SUPENNS001R01: 17 train, 2 val, 2 test\n",
      "Loading splits from P5_D3_Tsymptom_provocation_SUPENNS001R01.part2.pkl...\n",
      "Saved new splits for P5_D3_Tsymptom_provocation_SUPENNS001R01.part2: 15 train, 1 val, 1 test\n",
      "Loading splits from P5_D3_Tsymptom_provocation_SUPENNS001R01.part4.pkl...\n",
      "Saved new splits for P5_D3_Tsymptom_provocation_SUPENNS001R01.part4: 5 train, 0 val, 0 test\n",
      "Loading splits from P5_D3_Tsymptom_provocation_SUPENNS001R01.part3.pkl...\n",
      "Saved new splits for P5_D3_Tsymptom_provocation_SUPENNS001R01.part3: 8 train, 1 val, 1 test\n",
      "Loading splits from P5_D5_Tsymptom_provocation_SUPENNS001R01.part3.pkl...\n",
      "Saved new splits for P5_D5_Tsymptom_provocation_SUPENNS001R01.part3: 7 train, 0 val, 0 test\n",
      "Loading splits from P5_D3_Tsymptom_provocation_SUPENNS001R01.pkl...\n",
      "Saved new splits for P5_D3_Tsymptom_provocation_SUPENNS001R01: 13 train, 1 val, 1 test\n",
      "All splits complete for patient 5.\n"
     ]
    }
   ],
   "source": [
    "#cell to make split for label data, only run when we need to make new splits from label data\n",
    "# import os\n",
    "# import pickle\n",
    "# import numpy as np\n",
    "# from temporaldata import Interval\n",
    "\n",
    "# # === Configuration ===\n",
    "# TRAIN_FRAC = 0.8\n",
    "# VAL_FRAC   = 0.0\n",
    "# TEST_FRAC  = 0.2\n",
    "# # Choose patient number\n",
    "# PATIENT = '5'\n",
    "\n",
    "# # Paths\n",
    "# BASE_DIR  = os.getcwd()  # current working directory\n",
    "# LABEL_DIR = os.path.join(BASE_DIR, 'processed_data', 'labels')\n",
    "\n",
    "# # Process each pkl for this patient\n",
    "# for fname in os.listdir(LABEL_DIR):\n",
    "#     if not fname.startswith(f'P{PATIENT}_') or not fname.endswith('.pkl'):\n",
    "#         continue\n",
    "\n",
    "#     session_id = fname[:-4]  # strip '.pkl'\n",
    "#     path = os.path.join(LABEL_DIR, fname)\n",
    "#     print(f\"Loading intervals from {fname}...\")\n",
    "#     with open(path, 'rb') as f:\n",
    "#         iv_orig = pickle.load(f)\n",
    "\n",
    "#     # Total intervals\n",
    "#     n = len(iv_orig.start)\n",
    "#     # Determine counts (floor for val & test)\n",
    "#     test_count  = int(np.floor(n * TEST_FRAC))\n",
    "#     val_count   = int(np.floor(n * VAL_FRAC))\n",
    "#     train_count = n - val_count - test_count\n",
    "\n",
    "#     # Shuffle indices\n",
    "#     indices = np.random.permutation(n)\n",
    "#     train_idx = indices[:train_count]\n",
    "#     val_idx   = indices[train_count:train_count + val_count]\n",
    "#     test_idx  = indices[train_count + val_count:train_count + val_count + test_count]\n",
    "\n",
    "#     # Build split Intervals\n",
    "#     def slice_iv(iv, idxs):\n",
    "#         return Interval(\n",
    "#             start=iv.start[idxs],\n",
    "#             end=iv.end[idxs],\n",
    "#             label=iv.label[idxs],\n",
    "#             timekeys=['start', 'end']\n",
    "#         )\n",
    "\n",
    "#     iv_train = slice_iv(iv_orig, train_idx)\n",
    "#     iv_val   = slice_iv(iv_orig, val_idx)\n",
    "#     iv_test  = slice_iv(iv_orig, test_idx)\n",
    "\n",
    "#     # Prepare dictionary\n",
    "#     splits = {\n",
    "#         'train': {f'halpern_ocd/{session_id}': iv_train},\n",
    "#         'val':   {f'halpern_ocd/{session_id}': iv_val},\n",
    "#         'test':  {f'halpern_ocd/{session_id}': iv_test},\n",
    "#     }\n",
    "\n",
    "#     # Overwrite the original file with the splits dict\n",
    "#     with open(path, 'wb') as f:\n",
    "#         pickle.dump(splits, f)\n",
    "\n",
    "#     # Confirmation\n",
    "#     print(f\"Saved split for {session_id}: {train_count} train, {val_count} val, {test_count} test\")\n",
    "\n",
    "# print(f\"All splits complete for patient {PATIENT}.\")\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from temporaldata import Interval\n",
    "\n",
    "# === Config===\n",
    "TRAIN_FRAC = 0.8\n",
    "VAL_FRAC   = 0.1\n",
    "TEST_FRAC  = 0.1\n",
    "PATIENT    = '5'\n",
    "\n",
    "# Paths \n",
    "BASE_DIR  = os.getcwd()\n",
    "LABEL_DIR = os.path.join(BASE_DIR, 'processed_data', 'labels')\n",
    "\n",
    "def slice_iv(iv, idxs):\n",
    "    return Interval(\n",
    "        start=iv.start[idxs],\n",
    "        end=iv.end[idxs],\n",
    "        label=iv.label[idxs],\n",
    "        timekeys=['start', 'end']\n",
    "    )\n",
    "\n",
    "for fname in os.listdir(LABEL_DIR):\n",
    "    if not (fname.startswith(f'P{PATIENT}_') and fname.endswith('.pkl')):\n",
    "        continue\n",
    "\n",
    "    session_id = fname[:-4]  # strip \".pkl\"\n",
    "    pkl_path   = os.path.join(LABEL_DIR, fname)\n",
    "    print(f\"Loading splits from {fname}...\")\n",
    "\n",
    "    # 1) load the existing splits dict\n",
    "    with open(pkl_path, 'rb') as f:\n",
    "        old_splits = pickle.load(f)\n",
    "\n",
    "    # 2) gather all Interval objects into one combined Interval\n",
    "    all_intervals = []\n",
    "    for split in ('train', 'val', 'test'):\n",
    "        for iv in old_splits.get(split, {}).values():\n",
    "            all_intervals.append(iv)\n",
    "\n",
    "    starts = np.concatenate([iv.start for iv in all_intervals])\n",
    "    ends   = np.concatenate([iv.end   for iv in all_intervals])\n",
    "    labels = np.concatenate([iv.label for iv in all_intervals])\n",
    "\n",
    "    iv_orig = Interval(start=starts, end=ends, label=labels, timekeys=['start', 'end'])\n",
    "    n = len(iv_orig.start)\n",
    "\n",
    "    # 3) compute new split sizes\n",
    "    val_count   = int(np.floor(n * VAL_FRAC))\n",
    "    test_count  = int(np.floor(n * TEST_FRAC))\n",
    "    train_count = n - val_count - test_count\n",
    "\n",
    "    # 4) shuffle and partition\n",
    "    idxs      = np.random.permutation(n)\n",
    "    train_idx = idxs[:train_count]\n",
    "    val_idx   = idxs[train_count:train_count + val_count]\n",
    "    test_idx  = idxs[train_count + val_count:train_count + val_count + test_count]\n",
    "\n",
    "    iv_train = slice_iv(iv_orig, train_idx)\n",
    "    iv_val   = slice_iv(iv_orig, val_idx)\n",
    "    iv_test  = slice_iv(iv_orig, test_idx)\n",
    "\n",
    "    # 5) assemble the new splits dict (new one doesnt have the halpern_ocd prefix)\n",
    "    new_splits = {\n",
    "        'train': {session_id: iv_train},\n",
    "        'val':   {session_id: iv_val},\n",
    "        'test':  {session_id: iv_test},\n",
    "    }\n",
    "\n",
    "    # 6) overwrite the original .pkl\n",
    "    with open(pkl_path, 'wb') as f:\n",
    "        pickle.dump(new_splits, f)\n",
    "\n",
    "    print(f\"Saved new splits for {session_id}:\",\n",
    "          f\"{train_count} train,\", f\"{val_count} val,\", f\"{test_count} test\")\n",
    "\n",
    "print(f\"All splits complete for patient {PATIENT}.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1e7687de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splits found: ['train', 'val', 'test']\n",
      "train | session = P5_D4_Tsymptom_provocation_SUPENNS001R02.part3 | intervals = 8\n",
      "val   | session = P5_D4_Tsymptom_provocation_SUPENNS001R02.part3 | intervals = 0\n",
      "test  | session = P5_D4_Tsymptom_provocation_SUPENNS001R02.part3 | intervals = 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Cell to check the splits structure, validate the splits, and print counts.\n",
    "\"\"\"\n",
    "# 1) Point to your labels directory and a session to check\n",
    "label_dir = os.path.join(os.getcwd(), 'processed_data', 'labels')\n",
    "session_id = 'P5_D4_Tsymptom_provocation_SUPENNS001R02.part3'\n",
    "pkl_path = os.path.join(label_dir, f\"{session_id}.pkl\")\n",
    "\n",
    "# 2) Load the splits dict\n",
    "with open(pkl_path, 'rb') as f:\n",
    "    splits = pickle.load(f)\n",
    "\n",
    "# 3) Print out the structure and interval counts\n",
    "print(\"Splits found:\", list(splits.keys()))\n",
    "for split, split_dict in splits.items():\n",
    "    for sid, iv in split_dict.items():\n",
    "        count = len(iv.start) if hasattr(iv, 'start') else 'N/A'\n",
    "        print(f\"{split:<5} | session = {sid:<40} | intervals = {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "67ba9d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell to make data loaders\n",
    "\n",
    "# START HERE FOR MODEL TRAINING\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import yaml\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch_brain.data import Dataset, chain\n",
    "from torch_brain.data.collate import collate\n",
    "from torch_brain.data.sampler import (\n",
    "    RandomFixedWindowSampler,\n",
    "    SequentialFixedWindowSampler,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "cfg = yaml.safe_load(open(\"config.yaml\"))\n",
    "# cfg is a list with one element; that element has a key \"selection\" mapping to a dict,\n",
    "# and that dict has your \"sessions\" list.\n",
    "session_list = cfg[0][\"selection\"][0][\"sessions\"]\n",
    "\n",
    "# 2) Define file roots (update these to where your files actually live)\n",
    "base_root  = \"/vol/cortex/cd3/pesaranlab/OCD_Mapping_Foundation\"\n",
    "h5_root    = os.path.join(\n",
    "    base_root,\n",
    "    \"processed_data_upd\",\n",
    "    \"processed_data\"\n",
    ")\n",
    "label_root = os.path.join(\n",
    "    base_root,\n",
    "    \"processed_data\",\n",
    "    \"labels\"\n",
    ")\n",
    "\n",
    "# 3) Create the Dataset object\n",
    "total_dataset = Dataset(\n",
    "    root=h5_root,\n",
    "    split=None,\n",
    "    config=\"config.yaml\",  \n",
    "    # drop the subject prefix — only use the session.id\n",
    "    session_id_prefix_fn=lambda d: \"\",\n",
    ")   \n",
    "\n",
    "\n",
    "\n",
    "# 4) Read each .pkl to collect train/val/test intervals\n",
    "sampling_intervals = {\"train\": {}, \"val\": {}, \"test\": {}}\n",
    "\n",
    "for session_id in session_list:\n",
    "    pkl_path = os.path.join(label_root, session_id + \".pkl\")\n",
    "    with open(pkl_path, \"rb\") as f:\n",
    "        splits = pickle.load(f)\n",
    "\n",
    "    for split in (\"train\", \"val\", \"test\"):\n",
    "        iv = splits.get(split, {}).get(session_id)\n",
    "        if iv is not None:\n",
    "            sampling_intervals[split][session_id] = iv\n",
    "\n",
    "\n",
    "# 5) Hyperparameters (hard‑coded)\n",
    "batch_size     = 10\n",
    "num_workers    = 1\n",
    "prefetch_factor= 1\n",
    "window_length  = 1.0  # seconds\n",
    "\n",
    "# 6) Create samplers for each split\n",
    "train_sampler = RandomFixedWindowSampler(\n",
    "    sampling_intervals=sampling_intervals[\"train\"],\n",
    "    window_length=1,\n",
    "    generator=torch.Generator().manual_seed(0),\n",
    ")\n",
    "val_sampler = SequentialFixedWindowSampler(\n",
    "    sampling_intervals=sampling_intervals[\"val\"],\n",
    "    window_length=1,\n",
    ")\n",
    "test_sampler = SequentialFixedWindowSampler(\n",
    "    sampling_intervals=sampling_intervals[\"test\"],\n",
    "    window_length=1,\n",
    ")\n",
    "\n",
    "# 7) Wrap samplers in DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    total_dataset, batch_size=batch_size, sampler=train_sampler,\n",
    "    collate_fn=collate, num_workers=num_workers,\n",
    "    drop_last=True, persistent_workers=True,\n",
    "    prefetch_factor=prefetch_factor,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    total_dataset, batch_size=batch_size, sampler=val_sampler,\n",
    "    collate_fn=collate, num_workers=num_workers,\n",
    "    drop_last=False, persistent_workers=True,\n",
    "    prefetch_factor=prefetch_factor,\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    total_dataset, batch_size=batch_size, sampler=test_sampler,\n",
    "    collate_fn=collate, num_workers=num_workers,\n",
    "    drop_last=False, persistent_workers=True,\n",
    "    prefetch_factor=prefetch_factor,\n",
    ")\n",
    "\n",
    "# — now train_loader / val_loader / test_loader pull exactly from:\n",
    "#    - HD5 files under processed_data_upd\n",
    "#    - interval splits under processed_data/labels\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# for session in total_dataset.get_session_ids():\n",
    "#     with open(filename, 'rb') as f:\n",
    "#         intervals = pickle.load(f)\n",
    "#         train_sampling_intervals[session] = intervals['train'][session]\n",
    "\n",
    "# # Define train/val/test data loaders\n",
    "# train_sampler = RandomFixedWindowSampler(\n",
    "#     sampling_intervals=train_sampling_intervals,\n",
    "#     window_length=1.0,  # seconds\n",
    "#     generator=torch.Generator().manual_seed(0),\n",
    "# )\n",
    "# train_loader = DataLoader(\n",
    "#     dataset=total_dataset,\n",
    "#     batch_size=10,\n",
    "#     sampler=train_sampler,\n",
    "#     collate_fn=collate,\n",
    "#     num_workers=1,\n",
    "#     drop_last=True,\n",
    "#     persistent_workers=True,\n",
    "#     prefetch_factor=1,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "994aa961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sessions:\n",
      "   P5_D3_Tsymptom_provocation_SUPENNS001R01\n",
      "   P5_D3_Tsymptom_provocation_SUPENNS001R01.part2\n",
      "   P5_D3_Tsymptom_provocation_SUPENNS001R01.part3\n",
      "   P5_D3_Tsymptom_provocation_SUPENNS001R01.part4\n",
      "   P5_D4_TSymptpm_prov_SUPENNS001R02\n",
      "   P5_D4_TSymptpm_prov_SUPENNS001R02.part2\n",
      "   P5_D4_TSymptpm_prov_SUPENNS001R02.part3\n",
      "   P5_D5_Tsymptom_provocation_SUPENNS001R01\n",
      "   P5_D5_Tsymptom_provocation_SUPENNS001R01.part2\n",
      "   P5_D5_Tsymptom_provocation_SUPENNS001R01.part3\n"
     ]
    }
   ],
   "source": [
    "# Diagnostic — list every recording your Dataset knows about\n",
    "print(\"Dataset sessions:\")\n",
    "for sid in total_dataset.get_session_ids():\n",
    "    print(\"  \", sid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2ae7525d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train keys: dict_keys(['P5_D4_Tsymptom_provocation_SUPENNS001R02'])\n"
     ]
    }
   ],
   "source": [
    "with open(\"labels_jonathan/P5_D4_Tsymptom_provocation_SUPENNS001R02.pkl\", \"rb\") as f:\n",
    "    d = pickle.load(f)\n",
    "print(\"train keys:\", d[\"train\"].keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0c8765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling_intervals keys: dict_keys(['train', 'val', 'test'])\n",
      "  train sessions: ['P5_D5_Tsymptom_provocation_SUPENNS001R01', 'P5_D5_Tsymptom_provocation_SUPENNS001R01.part3', 'P5_D5_Tsymptom_provocation_SUPENNS001R01.part2', 'P5_D4_Tsymptom_provocation_SUPENNS001R02', 'P5_D4_Tsymptom_provocation_SUPENNS001R02.part3', 'P5_D4_Tsymptom_provocation_SUPENNS001R02.part2', 'P5_D3_Tsymptom_provocation_SUPENNS001R01', 'P5_D3_Tsymptom_provocation_SUPENNS001R01.part4', 'P5_D3_Tsymptom_provocation_SUPENNS001R01.part3', 'P5_D3_Tsymptom_provocation_SUPENNS001R01.part2']\n",
      "  # train sessions: 10\n",
      "Session P5_D5_Tsymptom_provocation_SUPENNS001R01:\n",
      "  # intervals = 17\n",
      "  Example durations (first 5) = [60.0, 60.0, 60.0, 60.0, 60.0]\n",
      "  Min = 60.0, Max = 60.0, Mean = 60.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell — inspect train interval durations\n",
    "\n",
    "# — Diagnostic cell — verify sampling_intervals exists and has data\n",
    "try:\n",
    "    print(\"sampling_intervals keys:\", sampling_intervals.keys())\n",
    "    print(\"  train sessions:\", list(sampling_intervals[\"train\"].keys()))\n",
    "    print(\"  # train sessions:\", len(sampling_intervals[\"train\"]))\n",
    "except NameError:\n",
    "    print(\"sampling_intervals is not defined in this scope.\")\n",
    "\n",
    "\n",
    "for session_id, intervals in sampling_intervals[\"train\"].items():\n",
    "    # compute duration of each interval\n",
    "    durations = [end - start for start, end in intervals]\n",
    "    if not durations:\n",
    "        print(f\"{session_id}: no intervals\")\n",
    "        continue\n",
    "    print(f\"Session {session_id}:\")\n",
    "    print(f\"  # intervals = {len(durations)}\")\n",
    "    print(f\"  Example durations (first 5) = {durations[:5]}\")\n",
    "    print(f\"  Min = {min(durations)}, Max = {max(durations)}, Mean = {sum(durations)/len(durations):.3f}\")\n",
    "    print()\n",
    "    # break after first session if you only want one example\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2d49ac36",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/jmehrotra/jmehrotra_ocd_venv/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/home/jmehrotra/jmehrotra_ocd_venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/jmehrotra/jmehrotra_ocd_venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/jmehrotra/jmehrotra_ocd_venv/lib/python3.10/site-packages/torch_brain/data/dataset.py\", line 457, in __getitem__\n    sample = self.get(index.recording_id, index.start, index.end)\n  File \"/home/jmehrotra/jmehrotra_ocd_venv/lib/python3.10/site-packages/torch_brain/data/dataset.py\", line 298, in get\n    data = copy.copy(self._data_objects[recording_id])\nKeyError: 'P5_D4_Tsymptom_provocation_SUPENNS001R02.part3'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m train_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(train_loader)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 4\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loader is empty—check your sampler and intervals.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/jmehrotra_ocd_venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:733\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 733\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    739\u001b[0m ):\n",
      "File \u001b[0;32m~/jmehrotra_ocd_venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1515\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1513\u001b[0m worker_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info\u001b[38;5;241m.\u001b[39mpop(idx)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1514\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rcvd_idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1515\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworker_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jmehrotra_ocd_venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1550\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data, worker_idx)\u001b[0m\n\u001b[1;32m   1548\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1549\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1550\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/jmehrotra_ocd_venv/lib/python3.10/site-packages/torch/_utils.py:750\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    746\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments or otherwise can't\u001b[39;00m\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;66;03m# be constructed, don't try to instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    749\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mKeyError\u001b[0m: Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/jmehrotra/jmehrotra_ocd_venv/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/home/jmehrotra/jmehrotra_ocd_venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/jmehrotra/jmehrotra_ocd_venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/jmehrotra/jmehrotra_ocd_venv/lib/python3.10/site-packages/torch_brain/data/dataset.py\", line 457, in __getitem__\n    sample = self.get(index.recording_id, index.start, index.end)\n  File \"/home/jmehrotra/jmehrotra_ocd_venv/lib/python3.10/site-packages/torch_brain/data/dataset.py\", line 298, in get\n    data = copy.copy(self._data_objects[recording_id])\nKeyError: 'P5_D4_Tsymptom_provocation_SUPENNS001R02.part3'\n"
     ]
    }
   ],
   "source": [
    "# Cell — run one iteration of train_loader\n",
    "train_iter = iter(train_loader)\n",
    "try:\n",
    "    batch = next(train_iter)\n",
    "except StopIteration:\n",
    "    raise RuntimeError(\"train_loader is empty—check your sampler and intervals.\")\n",
    "\n",
    "# If your Dataset returns (inputs, labels):\n",
    "if isinstance(batch, (list, tuple)) and len(batch) == 2:\n",
    "    inputs, labels = batch\n",
    "    print(\"Inputs type:\", type(inputs), \"shape:\", getattr(inputs, \"shape\", None))\n",
    "    print(\"Labels type:\", type(labels), \"shape:\", getattr(labels, \"shape\", None))\n",
    "# If your Dataset returns a dict:\n",
    "elif isinstance(batch, dict):\n",
    "    for k, v in batch.items():\n",
    "        print(f\"{k}: type={type(v)}, shape={getattr(v, 'shape', None)}\")\n",
    "else:\n",
    "    print(\"Batch content:\", batch)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jmehrotra_ocd_venv)",
   "language": "python",
   "name": "jmehrotra_ocd_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
