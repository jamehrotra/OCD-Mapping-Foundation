{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ee61f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Cell 1:\n",
    "Final script to convert labels (in matlab format) to pkl files, with 60s before timestamps\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from scipy.io import loadmat\n",
    "import h5py\n",
    "from temporaldata import RegularTimeSeries, Interval\n",
    "    \n",
    "# === Paths ===\n",
    "BASE_DIR  = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"Matlab_data_processed\")\n",
    "LABEL_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"processed_data\", \"labels\")\n",
    "os.makedirs(LABEL_DIR, exist_ok=True)\n",
    "\n",
    "# === MATLAB struct helper for HDF5 mats ===\n",
    "class MatlabStruct:\n",
    "    def __init__(self, entries):\n",
    "        for k, v in entries.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "# === Load MATLAB ftdata (v7.2 & v7.3) ===\n",
    "def load_ftdata(patient: str, session: str):\n",
    "    mat_path = os.path.join(BASE_DIR, patient, f\"{session}.mat\")\n",
    "    try:\n",
    "        mat = loadmat(mat_path, struct_as_record=False, squeeze_me=True)\n",
    "        ft = mat.get('ftdata') or mat.get('ftData')\n",
    "        if ft is None:\n",
    "            raise KeyError(\"ftdata not found\")\n",
    "        return ft\n",
    "    except Exception:\n",
    "        with h5py.File(mat_path, 'r') as f:\n",
    "            grp = f.get('ftdata') or f.get('ftData')\n",
    "            if grp is None:\n",
    "                raise KeyError(\"ftdata not found\")\n",
    "            trial_refs = grp['trial'][()]\n",
    "            fs         = float(np.array(grp['fsample']).item())\n",
    "            won_grp    = grp['wonideets']\n",
    "            trial_list = [np.array(f[r]).T for r in trial_refs.flatten()]\n",
    "            # behavioral fields (including timestamp)\n",
    "            won = {}\n",
    "            for fld in [\n",
    "                'ratingObsession', 'ratingCompulsion', 'ratingAnxiety',\n",
    "                'ratingEnergy', 'ratingDepressions', 'ratingDistress',\n",
    "                'isstim', 'timestamp'\n",
    "            ]:\n",
    "                arr = won_grp.get(fld)\n",
    "                won[fld] = np.array(arr).flatten() if arr is not None else None\n",
    "            return MatlabStruct({\n",
    "                'trial': trial_list,\n",
    "                'wonideets': MatlabStruct(won),\n",
    "                'fsample': fs\n",
    "            })\n",
    "\n",
    "# === Process a single patient/session ===\n",
    "def process_session(patient: str, session: str):\n",
    "    ft     = load_ftdata(patient, session)\n",
    "    trials = ft.trial\n",
    "    fs     = ft.fsample\n",
    "\n",
    "    # build full-time series\n",
    "    data = np.concatenate(trials, axis=1).T  # samples x channels\n",
    "    rt   = RegularTimeSeries(\n",
    "        raw=data,\n",
    "        sampling_rate=fs,\n",
    "        domain=Interval(0.0, data.shape[0] / fs)\n",
    "    )\n",
    "\n",
    "    # compute intervals from timestamps (timestamp = end sample index)\n",
    "    ts = getattr(ft.wonideets, 'timestamp', None)\n",
    "    if ts is not None and len(ts) > 0:\n",
    "        ts = np.asarray(ts, dtype=float)\n",
    "        # ends in seconds\n",
    "        ends   = ts / fs\n",
    "        # starts are 60 seconds before each end\n",
    "        starts = (ts - 60 * fs) / fs\n",
    "    else:\n",
    "        # fallback: equal-duration intervals from ftdata.time\n",
    "        times = getattr(ft, 'time', None)\n",
    "        if times:\n",
    "            n    = len(times)\n",
    "            dur  = times[0][-1] - times[0][0]\n",
    "            starts = np.arange(n) * dur\n",
    "            ends   = starts + dur\n",
    "        else:\n",
    "            starts = np.array([0.])\n",
    "            ends   = np.array([0.])\n",
    "\n",
    "    # extract ratings and stim flag\n",
    "    won         = ft.wonideets\n",
    "    rating_keys = [\n",
    "        'ratingObsession', 'ratingCompulsion', 'ratingAnxiety',\n",
    "        'ratingEnergy', 'ratingDepressions', 'ratingDistress'\n",
    "    ]\n",
    "    ratings = []\n",
    "    for key in rating_keys:\n",
    "        arr = getattr(won, key, None)\n",
    "        if arr is not None and len(arr) == len(starts):\n",
    "            ratings.append(np.asarray(arr, dtype=float))\n",
    "        else:\n",
    "            ratings.append(np.full(len(starts), np.nan))\n",
    "    stim_arr = np.asarray(getattr(won, 'isstim', np.zeros(len(starts))), dtype=int)\n",
    "\n",
    "    # stack labels and build Interval\n",
    "    label_arr = np.stack(ratings + [stim_arr], axis=1)\n",
    "    iv = Interval(\n",
    "        start=starts,\n",
    "        end=ends,\n",
    "        label=label_arr,\n",
    "        timekeys=['start', 'end']\n",
    "    )\n",
    "\n",
    "    # save Interval pickle with new suffix\n",
    "    out_file = os.path.join(\n",
    "        LABEL_DIR,\n",
    "        f\"{patient}_{session}_intervals_new.pkl\"\n",
    "    )\n",
    "    with open(out_file, 'wb') as pf:\n",
    "        pickle.dump(iv, pf)\n",
    "    print(f\"Saved: {out_file}\")\n",
    "\n",
    "    # preview dump\n",
    "    loaded = pickle.load(open(out_file, 'rb'))\n",
    "    print(f\"Preview of {patient}_{session}_intervals_new:\")\n",
    "    print(\n",
    "        \"  starts[:3]:\", loaded.start[:3],\n",
    "        \"ends[:3]:\", loaded.end[:3],\n",
    "        \"labels[0]:\", loaded.label[0]\n",
    "    )\n",
    "\n",
    "    return rt, iv\n",
    "\n",
    "# === Batch Processing ===\n",
    "def main(patients=None):\n",
    "    if patients is None:\n",
    "        # sort Patient1, Patient2, ... numerically\n",
    "        patients = sorted(\n",
    "            [d for d in os.listdir(BASE_DIR) if d.startswith(\"Patient\")],\n",
    "            key=lambda n: int(n.replace(\"Patient\", \"\"))\n",
    "        )\n",
    "    for patient in patients:\n",
    "        sessions = sorted(\n",
    "            f[:-4]\n",
    "            for f in os.listdir(os.path.join(BASE_DIR, patient))\n",
    "            if f.endswith('.mat')\n",
    "        )\n",
    "        for session in sessions:\n",
    "            try:\n",
    "                process_session(patient, session)\n",
    "            except Exception as e:\n",
    "                print(f\"Error {patient}/{session}: {e}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea765ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell 2:\n",
    "Script: works to split a large Interval object into smaller subfiles based on specified counts. Used to process if multiple sessions per file \n",
    "\"\"\"\n",
    "\n",
    "# === Configuration ===\n",
    "# Specify patient and day\n",
    "PATIENT = \"Patient2\"\n",
    "DAY     = \"day5\"\n",
    "# Specify how many intervals per subfile, in order\n",
    "split_counts = [2,15,5,5]  # [3,4,13] for a total of 20 intervals\n",
    "\n",
    "# Paths\n",
    "BASE_DIR   = \"/vol/cortex/cd3/pesaranlab/OCD_Mapping_Foundation\"\n",
    "INPUT_DIR  = os.path.join(BASE_DIR, \"processed_data\", \"labels\")\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, \"labels_jonathan\")\n",
    "\n",
    "# Construct input filename\n",
    "filename    = f\"{PATIENT}_{DAY}_intervals_new.pkl\"\n",
    "input_path  = os.path.join(INPUT_DIR, filename)\n",
    "\n",
    "# Load the full Interval object\n",
    "with open(input_path, \"rb\") as f:\n",
    "    iv = pickle.load(f)\n",
    "\n",
    "total_intervals = len(iv.start)\n",
    "if sum(split_counts) != total_intervals:\n",
    "    raise ValueError(f\"Sum of split_counts ({sum(split_counts)}) != total intervals ({total_intervals})\")\n",
    "\n",
    "# Compute cumulative indices for slicing\n",
    "boundaries = np.cumsum([0] + split_counts)\n",
    "\n",
    "# Split and save\n",
    "for i, count in enumerate(split_counts, start=1):\n",
    "    start_idx = boundaries[i-1]\n",
    "    end_idx   = boundaries[i]\n",
    "    iv_sub = Interval(\n",
    "        start=iv.start[start_idx:end_idx],\n",
    "        end=iv.end[start_idx:end_idx],\n",
    "        label=iv.label[start_idx:end_idx],\n",
    "        timekeys=[\"start\",\"end\"]\n",
    "    )\n",
    "    out_name = f\"{PATIENT}_{DAY}_part{i}.pkl\"\n",
    "    out_path = os.path.join(OUTPUT_DIR, out_name)\n",
    "    with open(out_path, \"wb\") as of:\n",
    "        pickle.dump(iv_sub, of)\n",
    "    print(f\"Saved {count} intervals to {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbc0069",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell 3:\n",
    "Script that can be used to shift a sample by the correct amount of time.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# === Configuration ===\n",
    "# Specify the file, offset (in Hz), and sampling rate (Hz)\n",
    "PKL_FILE = \"/vol/cortex/cd3/pesaranlab/OCD_Mapping_Foundation/labels_jonathan/P2_D5_Tsymptom_provocation_SUPENNS001R03.pkl\"\n",
    "OFFSET_HZ = 2592480 + 4320120 + 888480 # how many samples to shift back\n",
    "FSAMPLE   = 1200.0  #sampling rate in Hz\n",
    "\n",
    "# 1) Compute the time offset in seconds\n",
    "offset_sec = OFFSET_HZ / FSAMPLE\n",
    "\n",
    "# 2) Load the existing Interval object\n",
    "with open(PKL_FILE, \"rb\") as f:\n",
    "    iv = pickle.load(f)\n",
    "\n",
    "# 3) Shift start and end times\n",
    "new_starts = iv.start - offset_sec\n",
    "new_ends   = iv.end   - offset_sec\n",
    "\n",
    "# 4) (Optional) warn if any intervals go negative\n",
    "n_neg = (new_starts < 0).sum()\n",
    "if n_neg:\n",
    "    print(f\"{n_neg} intervals start < 0 after shifting by {offset_sec:.3f}s\")\n",
    "\n",
    "# 5) Build a new shifted Interval\n",
    "iv_shifted = Interval(\n",
    "    start=new_starts,\n",
    "    end=new_ends,\n",
    "    label=iv.label,\n",
    "    timekeys=['start','end']\n",
    ")\n",
    "\n",
    "# 6) Overwrite the original file with the shifted version\n",
    "with open(PKL_FILE, \"wb\") as f:\n",
    "    pickle.dump(iv_shifted, f)\n",
    "\n",
    "print(f\"Shifted '{os.path.basename(PKL_FILE)}' by {offset_sec:.3f} seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f740c13",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# 1) Load the Interval object\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(PKL_PATH, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 9\u001b[0m     iv \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# 2) Inspect available attributes (optional)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded object type:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mtype\u001b[39m(iv))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Cell 4:\n",
    "\n",
    "quick script to load and inspect an Interval object from a .pkl file\n",
    "\n",
    "*assumes that pkl files do not have dicts yet.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# === Specify the path to your Interval .pkl file here ===\n",
    "PKL_PATH = \"/vol/cortex/cd3/pesaranlab/OCD_Mapping_Foundation/labels_jonathan/P2_D5_Tsymptom_provocation_SUPENNS001R03.pkl\"\n",
    "\n",
    "# 1) Load the Interval object\n",
    "with open(PKL_PATH, \"rb\") as f:\n",
    "    iv = pickle.load(f)\n",
    "\n",
    "# 2) Inspect available attributes (optional)\n",
    "print(\"Loaded object type:\", type(iv))\n",
    "print(\"Attributes:\", [a for a in dir(iv) if not a.startswith(\"_\")])\n",
    "\n",
    "# 3) Iterate and print each intervalâ€™s start, end, and labels\n",
    "n = len(iv.start)\n",
    "print(f\"\\nTotal intervals: {n}\\n\")\n",
    "for i in range(n):\n",
    "    start = iv.start[i]\n",
    "    end   = iv.end[i]\n",
    "    label = iv.label[i]\n",
    "    print(f\"Interval {i}: start={start}, end={end}, labels={label}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b04cb76",
   "metadata": {},
   "source": [
    "At this point, we have correct pkl files corresponding to sessions. \"excel_to_pkl.ipnyb\" ends at the same point. Both files meet here.\n",
    "\n",
    "Together, \"matlab_to_pkl\" and \"excel_to_pkl\" represent a preprocessing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "214c6bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading intervals from P3_D4_Tsymptom_provocation_SUPENNS001R02.pkl...\n",
      "Saved new splits to /vol/cortex/cd3/pesaranlab/OCD_Mapping_Foundation/labels_jonathan/P3_D4_Tsymptom_provocation_SUPENNS001R02.pkl: 24 train, 2 val, 2 test\n",
      "Loading intervals from P3_D5_Tsymptom_provocation_SUPENNS001R01.pkl...\n",
      "Saved new splits to /vol/cortex/cd3/pesaranlab/OCD_Mapping_Foundation/labels_jonathan/P3_D5_Tsymptom_provocation_SUPENNS001R01.pkl: 22 train, 2 val, 2 test\n",
      "Loading intervals from P3_D5_Tsymptom_provocation_SUPENNS001R01part2.pkl...\n",
      "Saved new splits to /vol/cortex/cd3/pesaranlab/OCD_Mapping_Foundation/labels_jonathan/P3_D5_Tsymptom_provocation_SUPENNS001R01part2.pkl: 23 train, 2 val, 2 test\n",
      "Loading intervals from P3_D3_Tsymptom_provocation_SUPENNS001R01part2.pkl...\n",
      "Saved new splits to /vol/cortex/cd3/pesaranlab/OCD_Mapping_Foundation/labels_jonathan/P3_D3_Tsymptom_provocation_SUPENNS001R01part2.pkl: 15 train, 1 val, 1 test\n",
      "Loading intervals from P3_D3_Tsymptom_provocation_SUPENNS001R02.pkl...\n",
      "Saved new splits to /vol/cortex/cd3/pesaranlab/OCD_Mapping_Foundation/labels_jonathan/P3_D3_Tsymptom_provocation_SUPENNS001R02.pkl: 16 train, 1 val, 1 test\n",
      "Loading intervals from P3_D3_Tsymptom_provocation_SUPENNS001R01.pkl...\n",
      "Saved new splits to /vol/cortex/cd3/pesaranlab/OCD_Mapping_Foundation/labels_jonathan/P3_D3_Tsymptom_provocation_SUPENNS001R01.pkl: 17 train, 2 val, 2 test\n",
      "Loading intervals from P3_D5_Tsymptom_provocation_SUPENNS001R01part3.pkl...\n",
      "  Removed 1 intervals (start<0 or end<60) from P3_D5_Tsymptom_provocation_SUPENNS001R01part3.pkl\n",
      "Saved new splits to /vol/cortex/cd3/pesaranlab/OCD_Mapping_Foundation/labels_jonathan/P3_D5_Tsymptom_provocation_SUPENNS001R01part3.pkl: 1 train, 0 val, 0 test\n",
      "Loading intervals from P3_D4_Tsymptom_provocation_SUPENNS001R03.pkl...\n",
      "Saved new splits to /vol/cortex/cd3/pesaranlab/OCD_Mapping_Foundation/labels_jonathan/P3_D4_Tsymptom_provocation_SUPENNS001R03.pkl: 9 train, 0 val, 0 test\n",
      "Loading intervals from P3_D4_Tsymptom_provocation_SUPENNS001R02part2.pkl...\n",
      "  Removed 1 intervals (start<0 or end<60) from P3_D4_Tsymptom_provocation_SUPENNS001R02part2.pkl\n",
      "Saved new splits to /vol/cortex/cd3/pesaranlab/OCD_Mapping_Foundation/labels_jonathan/P3_D4_Tsymptom_provocation_SUPENNS001R02part2.pkl: 17 train, 2 val, 2 test\n",
      "All splits complete for patient 3.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Cell 5: \n",
    "\n",
    "script to make splits from raw pkl files for a specific patient\n",
    "saves to labels_jonathan\n",
    "\n",
    "* Also filters out the intervals that have start < 0 or end < 60 seconds, as these are invalid.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from temporaldata import Interval\n",
    "\n",
    "# === Configuration ===\n",
    "TRAIN_FRAC = 0.8\n",
    "VAL_FRAC   = 0.1\n",
    "TEST_FRAC  = 0.1\n",
    "PATIENT    = '3'\n",
    "\n",
    "# Paths\n",
    "BASE_DIR   = \"/vol/cortex/cd3/pesaranlab/OCD_Mapping_Foundation\"\n",
    "INPUT_DIR  = os.path.join(BASE_DIR, \"processed_data\", \"labels\")\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, \"labels_jonathan\")  # already exists\n",
    "\n",
    "def slice_iv(iv, idxs):\n",
    "    return Interval(\n",
    "        start=iv.start[idxs],\n",
    "        end=iv.end[idxs],\n",
    "        label=iv.label[idxs],\n",
    "        timekeys=['start', 'end']\n",
    "    )\n",
    "\n",
    "# Process each pkl for this patient\n",
    "for fname in os.listdir(INPUT_DIR):\n",
    "    if not (fname.startswith(f'P{PATIENT}_') and fname.endswith('.pkl')):\n",
    "        continue\n",
    "\n",
    "    session_id = fname[:-4]  # strip \".pkl\"\n",
    "    in_path    = os.path.join(INPUT_DIR, fname)\n",
    "    out_path   = os.path.join(OUTPUT_DIR, fname)\n",
    "\n",
    "    print(f\"Loading intervals from {fname}...\")\n",
    "\n",
    "    # 1) load the original Interval object\n",
    "    with open(in_path, 'rb') as f:\n",
    "        iv_orig = pickle.load(f)\n",
    "\n",
    "    # --- Filter out invalid intervals ---\n",
    "    mask_valid = (iv_orig.start >= 0) & (iv_orig.end >= 60)\n",
    "    invalid_count = len(iv_orig.start) - int(mask_valid.sum())\n",
    "    if invalid_count > 0:\n",
    "        print(f\"  Removed {invalid_count} intervals (start<0 or end<60) from {fname}\")\n",
    "    iv_orig = Interval(\n",
    "        start=iv_orig.start[mask_valid],\n",
    "        end=iv_orig.end[mask_valid],\n",
    "        label=iv_orig.label[mask_valid],\n",
    "        timekeys=['start', 'end']\n",
    "    )\n",
    "\n",
    "    # 2) Determine split sizes\n",
    "    n = len(iv_orig.start)\n",
    "    val_count   = int(np.floor(n * VAL_FRAC))\n",
    "    test_count  = int(np.floor(n * TEST_FRAC))\n",
    "    train_count = n - val_count - test_count\n",
    "\n",
    "    # 3) Shuffle indices\n",
    "    indices   = np.random.permutation(n)\n",
    "    train_idx = indices[:train_count]\n",
    "    val_idx   = indices[train_count:train_count + val_count]\n",
    "    test_idx  = indices[train_count + val_count:train_count + val_count + test_count]\n",
    "\n",
    "    # 4) Build split Intervals\n",
    "    iv_train = slice_iv(iv_orig, train_idx)\n",
    "    iv_val   = slice_iv(iv_orig, val_idx)\n",
    "    iv_test  = slice_iv(iv_orig, test_idx)\n",
    "\n",
    "    # 5) Prepare splits dict\n",
    "    splits = {\n",
    "        'train': {f'halpern_ocd/{session_id}': iv_train},\n",
    "        'val':   {f'halpern_ocd/{session_id}': iv_val},\n",
    "        'test':  {f'halpern_ocd/{session_id}': iv_test},\n",
    "    }\n",
    "\n",
    "    # 6) Save to output directory\n",
    "    with open(out_path, 'wb') as f:\n",
    "        pickle.dump(splits, f)\n",
    "\n",
    "    print(f\"Saved new splits to {out_path}: {train_count} train, {val_count} val, {test_count} test\")\n",
    "\n",
    "print(f\"All splits complete for patient {PATIENT}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212a53dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading intervals from P5_D4_Tsymptom_provocation_SUPENNS001R02.part2.pkl...\n",
      "Saved new splits to /vol/cortex/cd3/pesaranlab/OCD_Mapping_Foundation/sequential_splits/P5_D4_Tsymptom_provocation_SUPENNS001R02.part2.pkl: 16 train, 1 val, 1 test\n",
      "Loading intervals from P5_D4_Tsymptom_provocation_SUPENNS001R02.part3.pkl...\n",
      "Saved new splits to /vol/cortex/cd3/pesaranlab/OCD_Mapping_Foundation/sequential_splits/P5_D4_Tsymptom_provocation_SUPENNS001R02.part3.pkl: 8 train, 0 val, 0 test\n",
      "Loading intervals from P5_D4_Tsymptom_provocation_SUPENNS001R02.pkl...\n",
      "Saved new splits to /vol/cortex/cd3/pesaranlab/OCD_Mapping_Foundation/sequential_splits/P5_D4_Tsymptom_provocation_SUPENNS001R02.pkl: 17 train, 2 val, 2 test\n",
      "Loading intervals from P5_D5_Tsymptom_provocation_SUPENNS001R01.part2.pkl...\n",
      "Saved new splits to /vol/cortex/cd3/pesaranlab/OCD_Mapping_Foundation/sequential_splits/P5_D5_Tsymptom_provocation_SUPENNS001R01.part2.pkl: 16 train, 2 val, 2 test\n",
      "Loading intervals from P5_D5_Tsymptom_provocation_SUPENNS001R01.pkl...\n",
      "Saved new splits to /vol/cortex/cd3/pesaranlab/OCD_Mapping_Foundation/sequential_splits/P5_D5_Tsymptom_provocation_SUPENNS001R01.pkl: 17 train, 2 val, 2 test\n",
      "Loading intervals from P5_D3_Tsymptom_provocation_SUPENNS001R01.part2.pkl...\n",
      "Saved new splits to /vol/cortex/cd3/pesaranlab/OCD_Mapping_Foundation/sequential_splits/P5_D3_Tsymptom_provocation_SUPENNS001R01.part2.pkl: 15 train, 1 val, 1 test\n",
      "Loading intervals from P5_D3_Tsymptom_provocation_SUPENNS001R01.part4.pkl...\n",
      "  Removed 1 intervals (start<0 or end<60) from P5_D3_Tsymptom_provocation_SUPENNS001R01.part4.pkl\n",
      "Saved new splits to /vol/cortex/cd3/pesaranlab/OCD_Mapping_Foundation/sequential_splits/P5_D3_Tsymptom_provocation_SUPENNS001R01.part4.pkl: 4 train, 0 val, 0 test\n",
      "Loading intervals from P5_D3_Tsymptom_provocation_SUPENNS001R01.part3.pkl...\n",
      "Saved new splits to /vol/cortex/cd3/pesaranlab/OCD_Mapping_Foundation/sequential_splits/P5_D3_Tsymptom_provocation_SUPENNS001R01.part3.pkl: 8 train, 1 val, 1 test\n",
      "Loading intervals from P5_D5_Tsymptom_provocation_SUPENNS001R01.part3.pkl...\n",
      "  Removed 1 intervals (start<0 or end<60) from P5_D5_Tsymptom_provocation_SUPENNS001R01.part3.pkl\n",
      "Saved new splits to /vol/cortex/cd3/pesaranlab/OCD_Mapping_Foundation/sequential_splits/P5_D5_Tsymptom_provocation_SUPENNS001R01.part3.pkl: 6 train, 0 val, 0 test\n",
      "Loading intervals from P5_D3_Tsymptom_provocation_SUPENNS001R01.pkl...\n",
      "Saved new splits to /vol/cortex/cd3/pesaranlab/OCD_Mapping_Foundation/sequential_splits/P5_D3_Tsymptom_provocation_SUPENNS001R01.pkl: 13 train, 1 val, 1 test\n",
      "All splits complete for patient 5.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Cell 5a: \n",
    "\n",
    "script to make sequential splits from raw pkl files for a specific patient\n",
    "saves to sequential_splits\n",
    "\n",
    "* Also filters out the intervals that have start < 0 or end < 60 seconds, as these are invalid.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from temporaldata import Interval\n",
    "\n",
    "# === Configuration ===\n",
    "TRAIN_FRAC = 0.8\n",
    "VAL_FRAC   = 0.1\n",
    "TEST_FRAC  = 0.1\n",
    "PATIENT    = '5'\n",
    "\n",
    "# Paths\n",
    "BASE_DIR   = \"/vol/cortex/cd3/pesaranlab/OCD_Mapping_Foundation\"\n",
    "INPUT_DIR  = os.path.join(BASE_DIR, \"processed_data\", \"labels\")\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, \"sequential_splits\")  # already exists\n",
    "\n",
    "def slice_iv(iv, idxs):\n",
    "    return Interval(\n",
    "        start=iv.start[idxs],\n",
    "        end=iv.end[idxs],\n",
    "        label=iv.label[idxs],\n",
    "        timekeys=['start', 'end']\n",
    "    )\n",
    "\n",
    "# Process each pkl for this patient\n",
    "for fname in os.listdir(INPUT_DIR):\n",
    "    if not (fname.startswith(f'P{PATIENT}_') and fname.endswith('.pkl')):\n",
    "        continue\n",
    "\n",
    "    session_id = fname[:-4]  # strip \".pkl\"\n",
    "    in_path    = os.path.join(INPUT_DIR, fname)\n",
    "    out_path   = os.path.join(OUTPUT_DIR, fname)\n",
    "\n",
    "    print(f\"Loading intervals from {fname}...\")\n",
    "\n",
    "    # 1) load the original Interval object\n",
    "    with open(in_path, 'rb') as f:\n",
    "        iv_orig = pickle.load(f)\n",
    "\n",
    "    # --- Filter out invalid intervals ---\n",
    "    mask_valid = (iv_orig.start >= 0) & (iv_orig.end >= 60)\n",
    "    invalid_count = len(iv_orig.start) - int(mask_valid.sum())\n",
    "    if invalid_count > 0:\n",
    "        print(f\"  Removed {invalid_count} intervals (start<0 or end<60) from {fname}\")\n",
    "    iv_orig = Interval(\n",
    "        start=iv_orig.start[mask_valid],\n",
    "        end=iv_orig.end[mask_valid],\n",
    "        label=iv_orig.label[mask_valid],\n",
    "        timekeys=['start', 'end']\n",
    "    )\n",
    "\n",
    "    # 2) Determine split sizes\n",
    "    n = len(iv_orig.start)\n",
    "    val_count   = int(np.floor(n * VAL_FRAC))\n",
    "    test_count  = int(np.floor(n * TEST_FRAC))\n",
    "    train_count = n - val_count - test_count\n",
    "\n",
    "    # 3) Sequential split (first TRAIN_FRAC, then VAL_FRAC, then TEST_FRAC)\n",
    "    indices   = np.arange(n)\n",
    "    train_idx = indices[:train_count]\n",
    "    val_idx   = indices[train_count:train_count + val_count]\n",
    "    test_idx  = indices[train_count + val_count:]\n",
    "\n",
    "    # 4) Build split Intervals\n",
    "    iv_train = slice_iv(iv_orig, train_idx)\n",
    "    iv_val   = slice_iv(iv_orig, val_idx)\n",
    "    iv_test  = slice_iv(iv_orig, test_idx)\n",
    "\n",
    "    # 5) Prepare splits dict\n",
    "    splits = {\n",
    "        'train': {f'halpern_ocd/{session_id}': iv_train},\n",
    "        'val':   {f'halpern_ocd/{session_id}': iv_val},\n",
    "        'test':  {f'halpern_ocd/{session_id}': iv_test},\n",
    "    }\n",
    "\n",
    "    # 6) Save to output directory\n",
    "    with open(out_path, 'wb') as f:\n",
    "        pickle.dump(splits, f)\n",
    "\n",
    "    print(f\"Saved new splits to {out_path}: {train_count} train, {val_count} val, {test_count} test\")\n",
    "\n",
    "print(f\"All splits complete for patient {PATIENT}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6a6dbc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading intervals from P5_D5_Tsymptom_provocation_SUPENNS001R01.part2.pkl...\n",
      "Saved new splits to /vol/cortex/cd3/pesaranlab/OCD_Mapping_Foundation/holdout_splits/P5_D5_Tsymptom_provocation_SUPENNS001R01.part2.pkl: 0 train, 0 val, 20 test\n",
      "Loading intervals from P5_D5_Tsymptom_provocation_SUPENNS001R01.pkl...\n",
      "Saved new splits to /vol/cortex/cd3/pesaranlab/OCD_Mapping_Foundation/holdout_splits/P5_D5_Tsymptom_provocation_SUPENNS001R01.pkl: 0 train, 0 val, 21 test\n",
      "Loading intervals from P5_D5_Tsymptom_provocation_SUPENNS001R01.part3.pkl...\n",
      "  Removed 1 intervals (start<0 or end<60) from P5_D5_Tsymptom_provocation_SUPENNS001R01.part3.pkl\n",
      "Saved new splits to /vol/cortex/cd3/pesaranlab/OCD_Mapping_Foundation/holdout_splits/P5_D5_Tsymptom_provocation_SUPENNS001R01.part3.pkl: 0 train, 0 val, 6 test\n",
      "All splits complete for patient 5 day 5.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Cell 5b: \n",
    "\n",
    "script to make splits from raw pkl files for a specific patient\n",
    "saves to labels_jonathan\n",
    "\n",
    "* Also filters out the intervals that have start < 0 or end < 60 seconds, as these are invalid.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from temporaldata import Interval\n",
    "\n",
    "# === Configuration ===\n",
    "TRAIN_FRAC = 0\n",
    "VAL_FRAC   = 0\n",
    "TEST_FRAC  = 1\n",
    "PATIENT    = '5'\n",
    "DAY = \"5\"\n",
    "\n",
    "# Paths\n",
    "BASE_DIR   = \"/vol/cortex/cd3/pesaranlab/OCD_Mapping_Foundation\"\n",
    "INPUT_DIR  = os.path.join(BASE_DIR, \"processed_data\", \"labels\")\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, \"holdout_splits\")  # already exists\n",
    "\n",
    "def slice_iv(iv, idxs):\n",
    "    return Interval(\n",
    "        start=iv.start[idxs],\n",
    "        end=iv.end[idxs],\n",
    "        label=iv.label[idxs],\n",
    "        timekeys=['start', 'end']\n",
    "    )\n",
    "\n",
    "# Process each pkl for this patient\n",
    "for fname in os.listdir(INPUT_DIR):\n",
    "    if not (fname.startswith(f'P{PATIENT}_D{DAY}') and fname.endswith('.pkl')):\n",
    "        continue\n",
    "\n",
    "    session_id = fname[:-4]  # strip \".pkl\"\n",
    "    in_path    = os.path.join(INPUT_DIR, fname)\n",
    "    out_path   = os.path.join(OUTPUT_DIR, fname)\n",
    "\n",
    "    print(f\"Loading intervals from {fname}...\")\n",
    "\n",
    "    # 1) load the original Interval object\n",
    "    with open(in_path, 'rb') as f:\n",
    "        iv_orig = pickle.load(f)\n",
    "\n",
    "    # --- Filter out invalid intervals ---\n",
    "    mask_valid = (iv_orig.start >= 0) & (iv_orig.end >= 60)\n",
    "    invalid_count = len(iv_orig.start) - int(mask_valid.sum())\n",
    "    if invalid_count > 0:\n",
    "        print(f\"  Removed {invalid_count} intervals (start<0 or end<60) from {fname}\")\n",
    "    iv_orig = Interval(\n",
    "        start=iv_orig.start[mask_valid],\n",
    "        end=iv_orig.end[mask_valid],\n",
    "        label=iv_orig.label[mask_valid],\n",
    "        timekeys=['start', 'end']\n",
    "    )\n",
    "\n",
    "    # 2) Determine split sizes\n",
    "    n = len(iv_orig.start)\n",
    "    val_count   = int(np.floor(n * VAL_FRAC))\n",
    "    test_count  = int(np.floor(n * TEST_FRAC))\n",
    "    train_count = n - val_count - test_count\n",
    "\n",
    "    # 3) Shuffle indices\n",
    "    indices   = np.random.permutation(n)\n",
    "    train_idx = indices[:train_count]\n",
    "    val_idx   = indices[train_count:train_count + val_count]\n",
    "    test_idx  = indices[train_count + val_count:train_count + val_count + test_count]\n",
    "\n",
    "    # 4) Build split Intervals\n",
    "    iv_train = slice_iv(iv_orig, train_idx)\n",
    "    iv_val   = slice_iv(iv_orig, val_idx)\n",
    "    iv_test  = slice_iv(iv_orig, test_idx)\n",
    "\n",
    "    # 5) Prepare splits dict\n",
    "    splits = {\n",
    "        'train': {f'halpern_ocd/{session_id}': iv_train},\n",
    "        'val':   {f'halpern_ocd/{session_id}': iv_val},\n",
    "        'test':  {f'halpern_ocd/{session_id}': iv_test},\n",
    "    }\n",
    "\n",
    "    # 6) Save to output directory\n",
    "    with open(out_path, 'wb') as f:\n",
    "        pickle.dump(splits, f)\n",
    "\n",
    "    print(f\"Saved new splits to {out_path}: {train_count} train, {val_count} val, {test_count} test\")\n",
    "\n",
    "print(f\"All splits complete for patient {PATIENT} day {DAY}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248ab275",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (3183398640.py, line 29)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[3], line 29\u001b[0;36m\u001b[0m\n\u001b[0;31m    timekeys=['start', 'end']P5_D5_Tsymptom_provocation_SUPENNS001R01\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Cell 6: \n",
    "\n",
    "\n",
    "cell to make splits from data that already has splits. Now also filters out splits \n",
    "\"\"\"\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from temporaldata import Interval\n",
    "\n",
    "# === Config===\n",
    "TRAIN_FRAC = 0.8\n",
    "VAL_FRAC   = 0.1\n",
    "TEST_FRAC  = 0.1\n",
    "PATIENT    = '5'\n",
    "\n",
    "# Paths \n",
    "BASE_DIR  = os.getcwd()\n",
    "LABEL_DIR = os.path.join(BASE_DIR, 'labels_jonathan')\n",
    "\n",
    "def slice_iv(iv, idxs):\n",
    "    return Interval(\n",
    "        start=iv.start[idxs],\n",
    "        end=iv.end[idxs],\n",
    "        label=iv.label[idxs],\n",
    "        timekeys=['start', 'end']P5_D5_Tsymptom_provocation_SUPENNS001R01\n",
    "    )\n",
    "\n",
    "for fname in os.listdir(LABEL_DIR):\n",
    "    if not (fname.startswith(f'P{PATIENT}_') and fname.endswith('.pkl')):\n",
    "        continue\n",
    "\n",
    "    session_id = fname[:-4]  # strip \".pkl\"\n",
    "    pkl_path   = os.path.join(LABEL_DIR, fname)\n",
    "    print(f\"Loading splits from {fname}...\")\n",
    "\n",
    "    # 1) load the existing splits dict\n",
    "    with open(pkl_path, 'rb') as f:\n",
    "        old_splits = pickle.load(f)\n",
    "\n",
    "    # 2) gather all Interval objects into one combined Interval\n",
    "    all_intervals = []\n",
    "    for split in ('train', 'val', 'test'):\n",
    "        for iv in old_splits.get(split, {}).values():\n",
    "            all_intervals.append(iv)\n",
    "\n",
    "    starts = np.concatenate([iv.start for iv in all_intervals])\n",
    "    ends   = np.concatenate([iv.end   for iv in all_intervals])\n",
    "    labels = np.concatenate([iv.label for iv in all_intervals])\n",
    "\n",
    "    iv_orig = Interval(start=starts, end=ends, label=labels, timekeys=['start', 'end'])\n",
    "\n",
    "\n",
    "\n",
    "    n = len(iv_orig.start)\n",
    "\n",
    "    # 3) compute new split sizes\n",
    "    val_count   = int(np.floor(n * VAL_FRAC))\n",
    "    test_count  = int(np.floor(n * TEST_FRAC))\n",
    "    train_count = n - val_count - test_count\n",
    "\n",
    "    # 4) shuffle and partition\n",
    "    idxs      = np.random.permutation(n)\n",
    "    train_idx = idxs[:train_count]\n",
    "    val_idx   = idxs[train_count:train_count + val_count]\n",
    "    test_idx  = idxs[train_count + val_count:train_count + val_count + test_count]\n",
    "\n",
    "    iv_train = slice_iv(iv_orig, train_idx)\n",
    "    iv_val   = slice_iv(iv_orig, val_idx)\n",
    "    iv_test  = slice_iv(iv_orig, test_idx)\n",
    "\n",
    "    # 5) assemble the new splits dict (new one doesnt have the halpern_ocd prefix)\n",
    "    splits = {\n",
    "        'train': {f'halpern_ocd/{session_id}': iv_train},\n",
    "        'val':   {f'halpern_ocd/{session_id}': iv_val},\n",
    "        'test':  {f'halpern_ocd/{session_id}': iv_test},\n",
    "    }\n",
    "\n",
    "    # 6) overwrite the original .pkl\n",
    "    with open(pkl_path, 'wb') as f:\n",
    "        pickle.dump(splits, f)\n",
    "\n",
    "    print(f\"Saved new splits for {session_id}:\",\n",
    "          f\"{train_count} train,\", f\"{val_count} val,\", f\"{test_count} test\")\n",
    "\n",
    "print(f\"All splits complete for patient {PATIENT}.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jmehrotra_ocd_venv)",
   "language": "python",
   "name": "jmehrotra_ocd_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
